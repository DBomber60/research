---
title: "hierarchical modelling"
author: David Reynolds
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```

## Motivating Question

Suppose we have results from several experiments on the effect of a certain drug. How should we use this data to estimate the drug's effect? What is the error of the estimate?

This is a specific example of a much more general question: how do we use results of scientific research and incorporate it into our own research?

To make this concrete, let us consider the following data set regarding the effect of a drug.


```{r, echo=FALSE}
data <- data.frame(
                drug = paste('Study',letters[1:8]),
                effect = c(28,  8, -3,  7, -1,  1, 18, 12), 
                sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
kable(data)
attach(data)
```


### Option A

Choose the best study. Clearly this is not optimal since you are not making use of the informatin you have; however, very smart people choose this option all the time.

### Option B

Use some results from statistics class. 

Our data consists of $(\bar{y}_j, \sigma_j)$ for $j \in 1,...,J$, where $\bar{y}_j$ is the mean effect from experiment $j$, and $\sigma_j$ its standard error. Let us consider the following two models, 

\begin{align}
\text{Model 1: }\bar{y}_j &\underset {ind}{\sim} N(\mu, \sigma_j) \\
\text{Model 2: }\bar{y}_j &\underset {ind}{\sim} N(\theta_j, \sigma_j)
\end{align}

Conceptually, **Model 1** assumes that each experiment provides an independent estimate of the drug's *true* effect, $\mu$. Under this model, we can obtain an estimator $\hat{\mu}$ by maximizing the likelihood of our data.

\begin{align}
\hat{\mu} &= \underset{\mu}{\arg\max} \prod_j (2 \pi \sigma_j^2)^{-1/2} \exp \bigg(- \frac{( \bar{y}_j -\mu )^2 }{2 \sigma_j^2} \bigg) \\
&= \frac{\sum_j \bar{y}_j/ \sigma_j^2 }{\sum_j 1/ \sigma_j^2}
\end{align}

```{r}
sum(effect/sigma^2)/sum(1/sigma^2) # mu_hat
```

This seems okay. However, because the experimental conditions, for example the age or other attributes of the test subjects, length of the experiment and so on, are likely to affect the results, it does not feel right to assume the are no differences at all between the groups - an assumption we make by assuming a common $\mu$. In statistician jargon, we would like to acknowledge the *unobserved heterogeneity* across groups (experiments).

While we are uncomfortable treating the effect parameter estimated by each experiment as being exactly equal ($\mu$), it also does not seem right to treat them as independent parameters, which would be the implied assumption under the **Model 2**. 

To further illustrate the problems with these approaches, consider the following statements:

1. The probability that the true effect in A is greater than $28.4$ is $1/2$ (implied by **Model 1**).
2. The probability that the true effect in A is less than $7.7$ is $1/2$ (implied by **Model 2**).

Is there a middle ground?

### Option C

Hierarchical modelling! 

The idea is to break the model down into smaller easier understood pieces (levels), which when put together describes the joint distribution of all data and parameters. In this case, we can think about study level parameters on one level (i.e., we can denote $\theta_j$ as the true effect of study $j$) and the the sampling distribution of the study-specific data on another level. Futhermore, we can assume the study parameters are a sample from an underlying population distribution, and the variance of this population distribution, which is estimated from the data, determines how much the parameters of the sampling distribution are shrunk towards the common mean.

\begin{align}
\theta_j &\sim N(\mu, \tau) \\
\bar{y}_j \vert \theta_j &\sim N(\theta_j, \sigma_j)
\end{align}

In order to make this hierarchical model a Bayesian hierarchical model, we also specify a prior distribution for the **hyperparameters**, $p(\mu, \tau)$. A key conceptual difference between this model setup and those in Option B is the treatment of parameters $(\theta_1,...,\theta_J,\mu,\tau)$ as random variables. This is the defining characteristic of Bayesian statistics. Despite the conceptual newness, this framework subsumes the previous two.


\begin{align}
p(\mu) &\propto 1 \\
\bar{y}_j &\sim N(\mu, \sigma_j)
\end{align}

\begin{align}
p(\theta_j) &\propto 1 \text{ for } j \in \{1,...,J\} \\
\bar{y}_j &\sim N(\theta_j, \sigma_j)
\end{align}

















